{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A try of unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Baumâ€“Welch algorithm to estimate the parameters of our HMM, it is a type of EM algorithm, we set the parameters with random initial conditions at first, then we compute alpha(Forward procedure) and beta(Backward procedure), like what we do in supervised learning. To update the parametres, we have to compute 2 new temporary variables(gama and epsilon), then update according to these 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train10 :  29057\n",
      "length of train20 :  27184\n",
      "length of test10 :  1501\n",
      "length of test20 :  3374\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "train10 = pickle.load( open( \"./typos-data/train10.pkl\", \"rb\" ) )\n",
    "train20 = pickle.load( open( \"./typos-data/train20.pkl\", \"rb\" ) )\n",
    "test10 = pickle.load( open( \"./typos-data/test10.pkl\", \"rb\" ) )\n",
    "test20 = pickle.load( open( \"./typos-data/test20.pkl\", \"rb\" ) )\n",
    "print(\"length of train10 : \",len(train10))\n",
    "print(\"length of train20 : \",len(train20))\n",
    "print(\"length of test10 : \",len(test10))\n",
    "print(\"length of test20 : \",len(test20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list=[]\n",
    "state_list=[]\n",
    "for mot in train10:\n",
    "    for (l1,l2) in mot:\n",
    "        observation_list.append(l1)\n",
    "        state_list.append(l2)\n",
    "observation_list=list(set(observation_list))\n",
    "state_list=list(set(state_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros, multiply\n",
    "import numpy as np\n",
    "import sys\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# construc the first order HMM Model\n",
    "\n",
    "UNK = \"<unk>\"\n",
    "UNKid = 0\n",
    "epsilon = 1e-100\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list, train = None,test = None,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, smoothing_obs = 0.01):\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print(\"HMM creating with: \")\n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            print(str(self.N)+\" states\")\n",
    "            print(str(self.M)+\" observations\")\n",
    "            self.train=train\n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.N, self.M), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.data2index()\n",
    "#             self.calculer_pi()\n",
    "#             self.calculer_A()\n",
    "#             self.calculer_B()\n",
    "            self.Baum_Welch()\n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "        \n",
    "        def data2index(self):\n",
    "            self.Y_char={}\n",
    "            for i in self.Y_index:\n",
    "                self.Y_char[self.Y_index[i]]=i\n",
    "        \n",
    "#         def calculer_pi(self):\n",
    "#             for word in self.train:\n",
    "#                 self.initial_state_proba[self.Y_index[word[0][1]]]+=1\n",
    "# #             print(self.initial_state_proba)\n",
    "#             self.initial_state_proba/=len(self.train)\n",
    "    \n",
    "#         def calculer_A(self):\n",
    "#             s=0\n",
    "#             for word in self.train:\n",
    "#                 for i in range(len(word)-1):\n",
    "#                     s+=1\n",
    "#                     w1=self.Y_index[word[i][1]]\n",
    "#                     w2=self.Y_index[word[i+1][1]]\n",
    "#                     self.transition_proba[w1][w2]+=1\n",
    "#             tmp=self.transition_proba.T\n",
    "#             self.transition_proba=(tmp/self.transition_proba.sum(axis=1)).T\n",
    "# #             self.transition_proba/=self.transition_proba.sum(axis=0)\n",
    "        \n",
    "#         def calculer_B(self):\n",
    "#             s=0\n",
    "#             for word in self.train:\n",
    "#                 for i in range(len(word)):\n",
    "#                     s+=1\n",
    "#                     w1=self.Y_index[word[i][1]]\n",
    "#                     w2=self.X_index[word[i][0]]\n",
    "#                     self.observation_proba[w1][w2]+=1\n",
    "#             tmp=self.observation_proba.T\n",
    "#             self.observation_proba=(tmp/self.observation_proba.sum(axis=1)).T\n",
    "        def initialize_parametres(self):\n",
    "            self.transition_proba = np.random.rand(26, 26)\n",
    "            self.observation_proba = np.random.rand(26, 26)\n",
    "            self.initial_state_proba = np.random.rand(26,)\n",
    "            self.transition_proba/=self.transition_proba.sum(axis=1)\n",
    "            self.observation_proba/=self.observation_proba.sum(axis=1)\n",
    "            self.initial_state_proba/=self.initial_state_proba.sum()\n",
    "\n",
    "            \n",
    "        def calculer_alpha(self,word):            \n",
    "            alpha=np.zeros((len(self.Y_index),len(word)),float)\n",
    "            alpha[:,0]=self.initial_state_proba*self.observation_proba[:,self.X_index[word[0][0]]]\n",
    "            if len(word)==1:\n",
    "                return alpha\n",
    "            for i in range(1,len(word)):\n",
    "                alpha[:,i]=(alpha[:,i-1]*self.transition_proba.T).sum(axis=1)*\\\n",
    "                self.observation_proba[:,self.X_index[word[i][0]]]\n",
    "            return alpha\n",
    "        \n",
    "        def calculer_beta(self,word):\n",
    "            beta=np.zeros((len(self.Y_index),len(word)),float)\n",
    "            beta[:,-1]=1\n",
    "            if len(word)==1:\n",
    "                return beta\n",
    "            for i in range(len(word)-2,-1,-1):\n",
    "                beta[:,i]=(self.transition_proba*self.observation_proba[:,self.X_index[word[i+1][0]]]*\\\n",
    "                           beta[:,i+1]).sum(axis=1)\n",
    "            return beta\n",
    "        def calculer_epsilon(self,alpha,beta,word):\n",
    "            epsilon = np.zeros([26,26,len(word)])\n",
    "            for t in range(len(word)-1):\n",
    "                for i in range(26):\n",
    "                    for j in range(26):\n",
    "                        epsilon[i,j,t]=alpha[i,t]*self.transition_proba[i][j]*\\\n",
    "                        self.observation_proba[j,self.X_index[word[t+1][0]]]*beta[j,t+1]\\\n",
    "                \n",
    "                            \n",
    "            epsilon/=sum(sum(epsilon))\n",
    "            return epsilon\n",
    "        def para_update(self,word,gamma,epsilon):\n",
    "\n",
    "            self.initial_state_proba = gamma[:,0]\n",
    "            for i in range(26):\n",
    "                for j in range(26):\n",
    "                    self.transition_proba[i,j] = sum(epsilon[i,j,:]) / sum(gamma[i,:])\n",
    "\n",
    "            for j in range(26):\n",
    "                for k in range(26):\n",
    "                    self.observation_proba[j,k] = sum([gamma[j][t] for t in range(len(word)-1)\\\n",
    "                                                       if k == self.X_index[word[t][0]]])/sum(gamma[j,:])\n",
    "\n",
    "\n",
    "\n",
    "                        \n",
    "        def Baum_Welch(self):\n",
    "            self.initialize_parametres()\n",
    "            epochs=30\n",
    "            for i in range(epochs):\n",
    "                for word in self.train:\n",
    "                    if len(word)<2:\n",
    "                        continue\n",
    "                    alpha = self.calculer_alpha(word)\n",
    "                    beta = self.calculer_beta(word)\n",
    "                    gama = alpha * beta\n",
    "#                     gama/= gama.sum(axis=1)\n",
    "                    gama/=sum(gama)\n",
    "                    epsilon = self.calculer_epsilon(alpha,beta,word)\n",
    "                    gama=gama[:,:-1]\n",
    "                    self.para_update(word,gama,epsilon)\n",
    "\n",
    "                    \n",
    "        \n",
    "        def FB(self,alpha,beta):\n",
    "            prob = alpha * beta\n",
    "            index=list(prob.argmax(axis=0))\n",
    "#             print(index)\n",
    "            r=[self.Y_char[i] for i in index]\n",
    "            return r\n",
    "    \n",
    "        def viterbi(self,word):\n",
    "            N=len(self.Y_index)\n",
    "            delta=np.zeros(N, float)\n",
    "            delta_t=np.zeros(N, float)\n",
    "            tmp=np.zeros(N, float)\n",
    "            index=np.zeros((len(word),N), int)\n",
    "            delta=self.initial_state_proba*self.observation_proba[:,self.X_index[word[0][0]]]\n",
    "            for t in range(1, len(word)):\n",
    "                p=self.X_index[word[t][0]]\n",
    "                for j in range(N):\n",
    "                    tmp=delta*self.transition_proba[:,j]\n",
    "                    index[t,j]=tmp.argmax()\n",
    "                    delta_t[j]=tmp[index[t,j]]*self.observation_proba[j,p]\n",
    "                delta, delta_t = delta_t, delta\n",
    "            result=[delta.argmax()]\n",
    "            for i in index[-1:0:-1]:\n",
    "                result.append(i[result[-1]])\n",
    "            result.reverse()\n",
    "            r=[self.Y_char[i] for i in result]\n",
    "            return r\n",
    "        def score_FB(self, test):\n",
    "            s=0\n",
    "            e=0\n",
    "            correct=0\n",
    "            creat=0\n",
    "            for word in test:\n",
    "                alpha=self.calculer_alpha(word)\n",
    "                beta=self.calculer_beta(word)\n",
    "                nword=self.FB(alpha,beta)\n",
    "                for i,j in zip(word,nword):\n",
    "                    s+=1\n",
    "                    if i[1]!=j:\n",
    "                        e+=1\n",
    "                    if i[0]!=i[1] and i[1]==j:\n",
    "                        correct+=1\n",
    "                    if i[1]!=j and i[0]==i[1]:\n",
    "                        creat+=1\n",
    "            return {\"Error rate\":e/s, \"Errors correct\":correct, \"Errors creat\":creat}\n",
    "        \n",
    "        def score_viterbi(self, test):\n",
    "            s=0\n",
    "            e=0\n",
    "            correct=0\n",
    "            creat=0\n",
    "            for word in test:\n",
    "                nword=self.viterbi(word)\n",
    "                for i,j in zip(word,nword):\n",
    "                    s+=1\n",
    "                    if i[1]!=j:\n",
    "                        e+=1\n",
    "                    if i[0]!=i[1] and i[1]==j:\n",
    "                        correct+=1\n",
    "                    if i[1]!=j and i[0]==i[1]:\n",
    "                        creat+=1\n",
    "            return {\"Error rate\":e/s, \"Errors correct\":correct, \"Errors creat\":creat}\n",
    "        def baseline(self,test):\n",
    "            s=0\n",
    "            e=0\n",
    "            for word in test:\n",
    "                for l in word:\n",
    "                    s+=1\n",
    "                    if l[0]!=l[1]:\n",
    "                        e+=1\n",
    "            return e/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM creating with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gjx/.local/lib/python3.6/site-packages/ipykernel_launcher.py:134: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test10, trained with train10:\n",
      "Baseline: 0.10177595628415301\n",
      "FB for test10: {'Error rate': 0.9780054644808743, 'Errors correct': 13, 'Errors creat': 6427}\n",
      "Viterbe for test10: {'Error rate': 0.9780054644808743, 'Errors correct': 13, 'Errors creat': 6427}\n"
     ]
    }
   ],
   "source": [
    "model1=HMM(state_list,observation_list,train10,None)\n",
    "print(\"Test10, trained with train10:\")\n",
    "print(\"Baseline:\",model1.baseline(test10))\n",
    "print(\"FB for test10:\",model1.score_FB(test10))\n",
    "print(\"Viterbe for test10:\",model1.score_viterbi(test10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of computation is very long, and the results are very poor. Maybe the number of trainings is not enough, or it is more likely that somewhere of our program is wrong. Anyway, through this experiment, our understanding of unsupervised learning and EM algorithm is much deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
