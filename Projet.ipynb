{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwILkW1F8FnJ"
   },
   "source": [
    "# Project: Second-Order HMM for typos correction\n",
    "\n",
    "\n",
    "\n",
    "The goal is to design a model to correct typos in texts without a dictionaries.\n",
    "\n",
    "In this problem, a state refers to the correct letter that should have been typed, and an observation refers to the actual letter that is typed. Given a sequence of outputs/observations (i.e., actually typed letters), the problem is to reconstruct the hidden state sequence (i.e., the intended sequence of letters). Thus, data for this problem looks like:\n",
    "\n",
    "* [('t', 't'), ('h', 'h'), ('w', 'e'), ('k', 'm')]\n",
    "* [('f', 'f'), ('o', 'o'), ('r', 'r'), ('m', 'm')] \n",
    "\n",
    "The first example is misspelled: the observation is thwk while the correct word is them. The second example is correctly typed.\n",
    "\n",
    "Data for this problem was generated as follows: starting with a text document, in this case, the Unabomber's Manifesto, which was chosen not for political reasons, but for its convenience being available on-line and of about the right length, all numbers and punctuation were converted to white space and all letters converted to lower case. The remaining text is a sequence only over the lower case letters and the space character, represented in the data files by an underscore character. Next, typos were artificially added to the data as follows: with 90% probability, the correct letter is transcribed, but with 10% probability, a randomly chosen neighbor (on an ordinary physical keyboard) of the letter is transcribed instead. Space characters are always transcribed correctly. In a harder variant of the problem, the rate of errors is increased to 20%.\n",
    "\n",
    "The dataset in an archive, see the shared drive to download it. This archive contains 4 pickles: train10 and test10 constitute the dataset with 10% or spelling errors, while train20 and test20 the one with 20% or errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 : First Order HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtP9d0Pz8FnL"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "import sys\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None):\n",
    "            \"\"\"Builds a new Hidden Markov Model\n",
    "            state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            print(\"HMM created with: \")\n",
    "            self.N = len(state_list) # The number of states\n",
    "            self.M = len(observation_list) # The number of words in the vocabulary\n",
    "            print(str(self.N)+\" states\")\n",
    "            print(str(self.M)+\" observations\")\n",
    "            self.omega_Y = state_list # Keep the vocabulary of tags\n",
    "            self.omega_X = observation_list # Keep the vocabulary of tags\n",
    "            # Init. of the 3 distributions : observation, transition and initial states\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            # Since everything will be stored in numpy arrays, it is more convenient and compact to \n",
    "            # handle words and tags as indices (integer) for a direct access. However, we also need \n",
    "            # to keep the mapping between strings (word or tag) and indices. \n",
    "            self.make_indexes()\n",
    "\n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities arrays\"\"\"\n",
    "            self.Y_index = {}\n",
    "            omega_Y_keys = [key for key in self.omega_Y.keys()]\n",
    "            omega_X_keys = [key for key in self.omega_X.keys()]\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[omega_Y_keys[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[omega_X_keys[i]] = i\n",
    "        \n",
    "        def compute_init_state_proba(self, data):\n",
    "            for sent in data:\n",
    "                self.initial_state_proba[self.Y_index[sent[0][1]]]+=1\n",
    "            self.initial_state_proba/=len(data)\n",
    "            \n",
    "        def compute_observation_probas(self, data):            \n",
    "            for phr in data:\n",
    "                for word in phr:\n",
    "                    x = self.X_index[word[0]]\n",
    "                    y = self.Y_index[word[1]]\n",
    "                    self.observation_proba[x][y] += 1\n",
    "            self.observation_proba /= np.sum(self.observation_proba, axis=1)[:, np.newaxis]\n",
    "             \n",
    "        def compute_transition_probas(self, data):            \n",
    "            for phr in data:\n",
    "                for i in range(len(phr) - 1):\n",
    "                    yplus1 = self.Y_index[phr[i + 1][1]]\n",
    "                    y = self.Y_index[phr[i][1]]\n",
    "                    self.transition_proba[y][yplus1] += 1\n",
    "            self.transition_proba /= np.sum(self.transition_proba, axis=1)[:, np.newaxis]\n",
    "            \n",
    "        def init_parameters(self, train_set):\n",
    "            self.compute_init_state_proba(train_set)\n",
    "            self.compute_observation_probas(train_set)\n",
    "            self.compute_transition_probas(train_set)\n",
    "            \n",
    "        def forward(self, obs):\n",
    "            alpha = np.zeros((len(obs), len(self.Y_index)))\n",
    "            alpha[0] = self.initial_state_proba\\\n",
    "                        * self.observation_proba[self.X_index[obs[0][0]]]\n",
    "            for i in range(1, len(alpha)):\n",
    "                alpha[i] = self.observation_proba[self.X_index[obs[i][0]]] *\\\n",
    "                np.sum(self.transition_proba.T * alpha[i - 1], axis=1)\n",
    "            return alpha\n",
    "        \n",
    "        def backward(self, obs):\n",
    "            beta = np.zeros((len(obs), len(self.Y_index)))\n",
    "            beta[-1] = ones(len(self.Y_index))\n",
    "            for i in range(len(obs) - 2, -1, -1):\n",
    "                beta[i] = np.sum(beta[i + 1]\\\n",
    "                                 * self.observation_proba[self.X_index[obs[i + 1][0]]]\\\n",
    "                                 * self.transition_proba, axis=1)\n",
    "            return beta\n",
    "        \n",
    "        def decode(self, alpha, beta):\n",
    "            prob = alpha * beta\n",
    "            preds = prob.argmax(axis=1)\n",
    "            keys = [key for key in self.omega_Y.keys()]\n",
    "            return [keys[pred_ind] for pred_ind in preds]\n",
    "        \n",
    "        def viterbi(self, obs):\n",
    "            mu_max = np.zeros(len(obs))\n",
    "            tmp = self.initial_state_proba * self.observation_proba[self.X_index[obs[0][0]]]\n",
    "            index = [np.argmax(tmp)]\n",
    "            mu_max[0] = max(tmp)\n",
    "            for i in range(1, len(obs)):\n",
    "                tmp = self.observation_proba[self.X_index[obs[i][0]]]\\\n",
    "                        * self.transition_proba[self.Y_index[obs[i - 1][1]]]\\\n",
    "                        * mu_max[i - 1]\n",
    "                index.append(np.argmax(tmp))\n",
    "                mu_max[i] = max(tmp)\n",
    "            keys = [key for key in self.omega_Y.keys()]\n",
    "            return [keys[ind] for ind in index]\n",
    "            \n",
    "        def score_eval(self, test):\n",
    "            error = 0\n",
    "            elements = 0\n",
    "            errors_corrected = 0\n",
    "            errors_added = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                alpha = self.forward(word)\n",
    "                beta = self.backward(word)\n",
    "                preds = self.decode(alpha, beta)\n",
    "                elements += len(preds)\n",
    "                for x, y, pred in zip(base, truth, preds):\n",
    "                    if pred != x and pred == y:\n",
    "                        errors_corrected += 1\n",
    "                    if pred != y:\n",
    "                        error += 1\n",
    "                        if x == y:\n",
    "                            errors_added += 1\n",
    "            return error / elements, errors_corrected, errors_added\n",
    "        \n",
    "        def score_viterbi(self, test):\n",
    "            error = 0\n",
    "            elements = 0\n",
    "            errors_corrected = 0\n",
    "            errors_added = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                preds = self.viterbi(word)\n",
    "                elements += len(preds)\n",
    "                for x, y, pred in zip(base, truth, preds):\n",
    "                    if pred != x and pred == y:\n",
    "                        errors_corrected += 1\n",
    "                    if pred != y:\n",
    "                        error += 1\n",
    "                        if x == y:\n",
    "                            errors_added += 1\n",
    "            return error / elements, errors_corrected, errors_added\n",
    "\n",
    "        def score_dummy(self, test):\n",
    "            error = 0\n",
    "            elements = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                elements += len(truth)\n",
    "                for x, y in zip(base, truth):\n",
    "                    if x != y:\n",
    "                        error += 1\n",
    "            return error / elements\n",
    "        \n",
    "        def results_hmm(self, test):\n",
    "            error_test, fb_corrected, fb_added = self.score_eval(test)\n",
    "            viterbi_error_test, vit_corrected, vit_added = self.score_viterbi(test)\n",
    "            error_dummy = self.score_dummy(test)\n",
    "            print(\"Error forward-backward = {:.2%}, {} errors corrected, {} errors added\"\n",
    "                  .format(error_test, fb_corrected, fb_added))\n",
    "            print(\"Error viterbi = {:.2%}, {} errors corrected, {} errors added\"\n",
    "                  .format(viterbi_error_test, vit_corrected, vit_added))\n",
    "            print(\"Error with nothing changed = {:.2%}\".format(error_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture & séparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YO0E0GvD8FnS"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10 = pickle.load(open(\"./typos-data/train10.pkl\", \"rb\"))\n",
    "train20 = pickle.load(open(\"./typos-data/train20.pkl\", \"rb\"))\n",
    "test10 = pickle.load(open(\"./typos-data/test10.pkl\", \"rb\"))\n",
    "test20 = pickle.load(open(\"./typos-data/test20.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du vocabulaire & du HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrib_x_y_data(data):\n",
    "    set_tag = []\n",
    "    set_mot = []\n",
    "    dict_tag = dict()\n",
    "    dict_mot = dict()\n",
    "    for phrase in data:\n",
    "        for mot in phrase:\n",
    "            if not(mot[1] in set_tag):\n",
    "                set_tag.append(mot[1])\n",
    "                dict_tag[mot[1]]=0\n",
    "            dict_tag[mot[1]]+=1\n",
    "            if not(mot[0] in set_mot):\n",
    "                set_mot.append(mot[0])\n",
    "                dict_mot[mot[0]]=0\n",
    "            dict_mot[mot[0]]+=1\n",
    "    return dict_mot, dict_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus with 10% errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(train10)\n",
    "\n",
    "hmm = HMM(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error forward-backward = 8.29%, 332 errors corrected, 194 errors added\n",
      "Error viterbi = 8.92%, 296 errors corrected, 204 errors added\n",
      "Error with nothing changed = 10.18%\n"
     ]
    }
   ],
   "source": [
    "hmm.init_parameters(train10)\n",
    "\n",
    "hmm.results_hmm(test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus with 20% errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(train20)\n",
    "\n",
    "hmm_20 = HMM(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error forward-backward = 15.04%, 1453 errors corrected, 724 errors added\n",
      "Error viterbi = 15.85%, 1401 errors corrected, 808 errors added\n",
      "Error with nothing changed = 19.41%\n"
     ]
    }
   ],
   "source": [
    "hmm_20.init_parameters(train20)\n",
    "\n",
    "hmm_20.results_hmm(test20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Second order HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "import sys\n",
    "\n",
    "class HMM2(HMM):\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None):\n",
    "            super().__init__(state_list, observation_list)\n",
    "            self.observation_proba_order2 = zeros((self.M, self.N, self.N), float) \n",
    "            self.transition_proba_order2 = zeros((self.N, self.N, self.N), float) \n",
    "            \n",
    "        def compute_observation_probas_order2(self, data):            \n",
    "            for phr in data:\n",
    "                for i in range(1, len(phr)):\n",
    "                    x = self.X_index[phr[i][0]]\n",
    "                    y = self.Y_index[phr[i][1]]\n",
    "                    yminus1 = self.Y_index[phr[i - 1][1]]\n",
    "                    self.observation_proba_order2[x][yminus1][y] += 1\n",
    "            sumPlus1 = np.sum(self.observation_proba_order2, axis=2)[:, :, np.newaxis]\n",
    "            self.observation_proba_order2 /= np.where(sumPlus1 == 0, 1, sumPlus1)\n",
    "        \n",
    "        def compute_transition_probas_order2(self, data):  \n",
    "            nb_trigrams = 0.0\n",
    "            nb_bigrams = 0.0\n",
    "            unigram = zeros(self.N)\n",
    "            bigrams = zeros((self.N, self.N))\n",
    "            trigrams = zeros((self.N, self.N, self.N))\n",
    "            for phr in data:\n",
    "                for i in range(2, len(phr)):\n",
    "                    y = self.Y_index[phr[i][1]]\n",
    "                    unigram[y] += 1\n",
    "                    yminus1 = self.Y_index[phr[i - 1][1]]\n",
    "                    bigrams[yminus1][y] += 1\n",
    "                    nb_bigrams += 1.0\n",
    "                    yminus2 = self.Y_index[phr[i - 2][1]]\n",
    "                    trigrams[yminus2][yminus1][y] += 1\n",
    "                    nb_trigrams += 1.0\n",
    "                    \n",
    "            k3 = (np.log(trigrams + 1) + 999) / (np.log(trigrams + 1) + 1000)\n",
    "            k2 = (np.log(bigrams + 1) + 1) / (np.log(bigrams + 1) + 1000)\n",
    "            lambda1 = k3\n",
    "            lambda2 = (1 - k3) * k2\n",
    "            lambda3 = (1 - k3) * (1 - k2)\n",
    "            for phr in data:\n",
    "                for i in range(2, len(phr)):\n",
    "                    y = self.Y_index[phr[i][1]]\n",
    "                    yminus1 = self.Y_index[phr[i - 1][1]]\n",
    "                    yminus2 = self.Y_index[phr[i - 2][1]]\n",
    "                    self.transition_proba_order2[yminus2][yminus1][y] = lambda1[yminus2][yminus1][y] \\\n",
    "                                                * trigrams[yminus2][yminus1][y]\\\n",
    "                                                + lambda2[yminus2][yminus1][y] * bigrams[yminus1][y]\\\n",
    "                                                + lambda3[yminus2][yminus1][y] * unigram[y]\n",
    "                    \n",
    "            sumPlus1 = np.sum(self.transition_proba_order2, axis=1)[:, np.newaxis, :]\n",
    "            self.transition_proba_order2 /= np.where(sumPlus1 == 0, 1, sumPlus1)\n",
    "            \n",
    "        def init_parameters(self, train_set):\n",
    "            super().init_parameters(train_set)\n",
    "            self.compute_observation_probas_order2(train_set)\n",
    "            self.compute_transition_probas_order2(train_set)\n",
    "        \n",
    "        def viterbi(self, obs):\n",
    "            keys = [key for key in self.omega_Y.keys()]\n",
    "            \n",
    "            delta = np.zeros(len(obs))\n",
    "            tmp = self.initial_state_proba * self.observation_proba[self.X_index[obs[0][0]]]\n",
    "            phi = [np.argmax(tmp)]\n",
    "            delta[0] = max(tmp)\n",
    "            \n",
    "            if len(obs) < 2:\n",
    "                return [keys[ind] for ind in phi]\n",
    "            else:\n",
    "# For second order observation probabilities                \n",
    "#                 tmp2 = self.observation_proba_order2[self.X_index[obs[1][0]]][self.Y_index[obs[0][1]]]\\\n",
    "#                             * self.transition_proba_order1[self.Y_index[obs[0][1]]]\\\n",
    "#                             * delta[0]\n",
    "                tmp2 = self.observation_proba[self.X_index[obs[1][0]]]\\\n",
    "                            * self.transition_proba[self.Y_index[obs[0][1]]]\\\n",
    "                            * delta[0]\n",
    "                phi.append(np.argmax(tmp2))\n",
    "                delta[1] = max(tmp2)\n",
    "\n",
    "                for i in range(2, len(obs)):\n",
    "# For second order observation probabilities                \n",
    "#                     tmp = self.observation_proba_order2[self.X_index[obs[i][0]]][self.Y_index[obs[i - 1][1]]]\\\n",
    "#                             * self.transition_proba[self.Y_index[obs[i - 2][1]]][self.Y_index[obs[i - 1][1]]]\\\n",
    "#                             * delta[i - 1]\n",
    "                    tmp = self.observation_proba[self.X_index[obs[i][0]]]\\\n",
    "                        * self.transition_proba_order2[self.Y_index[obs[i - 2][1]]][self.Y_index[obs[i - 1][1]]]\\\n",
    "                        * delta[i - 1]\n",
    "                    phi.append(np.argmax(tmp))\n",
    "                    delta[i] = max(tmp)\n",
    "                return [keys[ind] for ind in phi]\n",
    "   \n",
    "        def results_hmm(self, test):\n",
    "            viterbi_error_test, vit_corrected, vit_added = self.score_viterbi(test)\n",
    "            error_dummy = self.score_dummy(test)\n",
    "            print(\"Error viterbi = {:.2%}, {} errors corrected, {} errors added\"\n",
    "                  .format(viterbi_error_test, vit_corrected, vit_added))\n",
    "            print(\"Error with nothing changed = {:.2%}\".format(error_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus with 10% errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(train10)\n",
    "\n",
    "hmm2 = HMM2(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm2.init_parameters(train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error viterbi = 6.82%, 355 errors corrected, 109 errors added\n",
      "Error with nothing changed = 10.18%\n"
     ]
    }
   ],
   "source": [
    "hmm2.results_hmm(test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus with 20% errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "26 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(train20)\n",
    "\n",
    "hmm2_20 = HMM2(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error viterbi = 12.35%, 1625 errors corrected, 447 errors added\n",
      "Error with nothing changed = 19.41%\n"
     ]
    }
   ],
   "source": [
    "hmm2_20.init_parameters(train20)\n",
    "\n",
    "hmm2_20.results_hmm(test20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Using first order observation probabilites (only knowing the current state) makes better results than observation probabilities knowing the current state and the previous state.\n",
    "- Weirdly, summing counts in transition probabilites for the axis 1, hence having the sum of probabilities equals to 1 for the state at time t - 1, instead of summing at 1 for the state at time t (axis 2), also provides better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling deletions and insertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, in order to be able to handle deletions and/or insertions, we need a training corpus possessing those two features.\n",
    "It can easily be created by randomly remove or add letters in our current corpus. We just need to specify how it is represented in the corpus. \n",
    "\n",
    "One solution would be to represent insertions with the observation being a letter and the tag the empty string, and the opposite works for deletion.\n",
    "\n",
    "For example : \n",
    "- Word cat with an inserted b at second position : [('c', 'c'), ('b', ''), ('a', 'a'), ('t', 't')]\n",
    "- Word cat with the t deleted : [('c', 'c'), ('a', 'a'), ('', 't')]\n",
    "\n",
    "#### How to handle this in the HMM ?\n",
    "\n",
    "The simplest solution would probably be to consider insertion and deletion as tags. Their counts will then be computed at the same time as everything else.\n",
    "\n",
    "In that case, knowing if we should add or remove a letter will also be decided in the same way, i.e. if their probability is the highest.\n",
    "\n",
    "For insertion, this should be enough by deleting the letter. But as for deletion, we also need to know which letter we should put where we decided that a letter was missing.\n",
    "\n",
    "One solution would be to see which is the most probable letter knowing the last states, or the last several states, but it won't probably be sufficient to have good results. One solution probably a bit better, would be to wait for reaching the end of the word and decide considering all observed states. Of course, this solution would also be harder to implement.\n",
    "\n",
    "The best solution would of course be to have a full dictionary of possible observations and match the observed words with it to find the correct missing letter. Obviously, this solution will only work if the number of possible observations is finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A try to handle noisy insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_insertions(dataset, insertion_probability):\n",
    "    set_tags = []\n",
    "    for word in dataset:\n",
    "        for letter in word:\n",
    "            if letter[1] not in set_tags:\n",
    "                set_tags.append(letter[1])\n",
    "    new_data = []\n",
    "    for word in dataset:\n",
    "        new_word = []\n",
    "        rand_insert = [np.random.random() < insertion_probability for _ in word]\n",
    "        for i, b in enumerate(rand_insert):\n",
    "            if b:\n",
    "                rand_letter = (set_tags[np.random.randint(len(set_tags))], '')\n",
    "                new_word.append(rand_letter)\n",
    "            new_word.append(word[i])\n",
    "        new_data.append(new_word)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "import sys\n",
    "\n",
    "class HMM3(HMM2):\n",
    "\n",
    "        \n",
    "        def score_viterbi(self, test):\n",
    "            error = 0\n",
    "            elements = 0\n",
    "            errors_corrected = 0\n",
    "            errors_added = 0\n",
    "            correctly_deleted = 0\n",
    "            uncorrectly_deleted = 0\n",
    "            noisy_notcorrected = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                preds = self.viterbi(word)\n",
    "                elements += len(preds)\n",
    "                for x, y, pred in zip(base, truth, preds):\n",
    "                    if pred != x and pred == y:\n",
    "                        if y == '':\n",
    "                            correctly_deleted += 1\n",
    "                        errors_corrected += 1\n",
    "                    if pred != y:\n",
    "                        error += 1\n",
    "                        if x == y:\n",
    "                            errors_added += 1\n",
    "                        if pred == '':\n",
    "                            uncorrectly_deleted += 1\n",
    "                        if y == '':\n",
    "                            noisy_notcorrected += 1\n",
    "            return error / elements, (uncorrectly_deleted + noisy_notcorrected) / elements,\\\n",
    "                    errors_corrected, errors_added, correctly_deleted, uncorrectly_deleted\n",
    "\n",
    "        def score_dummy(self, test):\n",
    "            error = 0\n",
    "            insertion_errors = 0\n",
    "            elements = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                elements += len(truth)\n",
    "                for x, y in zip(base, truth):\n",
    "                    if x != y:\n",
    "                        error += 1\n",
    "                        if y == '':\n",
    "                            insertion_errors += 1\n",
    "            return error / elements, insertion_errors / elements\n",
    "        \n",
    "        def results_hmm(self, test):\n",
    "            viterbi_error_test, vit_noise_errors, vit_corrected, vit_added, vit_noise, \\\n",
    "            vit_unc_noise = self.score_viterbi(test)\n",
    "            error_dummy, ins_errors_dummy = self.score_dummy(test)\n",
    "            print(\"Error viterbi = {:.2%}, including {:.2%} errors for correcting noisy insertions.\\\n",
    "            \\n{} errors corrected, {} errors added, {} noisy insertions removed, {} letters unjustly removed\"\n",
    "                  .format(viterbi_error_test, vit_noise_errors, vit_corrected, \\\n",
    "                          vit_added, vit_noise, vit_unc_noise))\n",
    "            print(\"Error with nothing changed = {:.2%}, noisy insertion errors = {:.2%}\"\n",
    "                  .format(error_dummy, ins_errors_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train10 = noisy_insertions(train10, 0.05)\n",
    "new_test10 = noisy_insertions(test10, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "27 states\n",
      "26 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(new_train10)\n",
    "\n",
    "hmm3 = HMM3(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm3.init_parameters(new_train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error viterbi = 10.62%, including 4.26% errors for correcting noisy insertions.            \n",
      "411 errors corrected, 118 errors added, 93 noisy insertions removed, 56 letters unjustly removed\n",
      "Error with nothing changed = 14.43%, noisy insertion errors = 4.74%\n"
     ]
    }
   ],
   "source": [
    "hmm3.results_hmm(new_test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "We think that the performance for removing noisy insertions will increase along the size of the train set. The more there are examples of transitions with noisy insertion, the more the chance of correctly removing those noisy insertions.\n",
    "As of now, it does work, but isn't really good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A try to handle characters deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_deletions(dataset, deletion_probability):\n",
    "    set_tags = []\n",
    "    for word in dataset:\n",
    "        for letter in word:\n",
    "            if letter[1] not in set_tags:\n",
    "                set_tags.append(letter[1])\n",
    "    new_data = []\n",
    "    for word in dataset:\n",
    "        new_word = []\n",
    "        rand_delete = [np.random.random() < deletion_probability for _ in word]\n",
    "        for i, b in enumerate(rand_delete):\n",
    "            if b:\n",
    "                new_word.append(('', word[i][1]))\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "        new_data.append(new_word)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from numpy import array, ones, zeros\n",
    "import sys\n",
    "\n",
    "class HMM4(HMM2):\n",
    "    \n",
    "        def score_viterbi(self, test):\n",
    "            error = 0\n",
    "            elements = 0\n",
    "            errors_corrected = 0\n",
    "            errors_added = 0\n",
    "            deletion_detected = 0\n",
    "            deletion_added = 0\n",
    "            deletion_undetected = 0\n",
    "            correctly_undeleted = 0\n",
    "            uncorrectly_undeleted = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                preds = self.viterbi(word)\n",
    "                elements += len(preds)\n",
    "                for x, y, pred in zip(base, truth, preds):\n",
    "                    if pred != x and pred == y:\n",
    "                        if x == '':\n",
    "                            correctly_undeleted += 1\n",
    "                            deletion_detected += 1\n",
    "                        errors_corrected += 1\n",
    "                    if pred != y:\n",
    "                        error += 1\n",
    "                        if x == y:\n",
    "                            errors_added += 1\n",
    "                        if x == '':\n",
    "                            if pred != '':\n",
    "                                deletion_detected += 1\n",
    "                            else:\n",
    "                                deletion_undetected += 1\n",
    "                            uncorrectly_undeleted += 1\n",
    "                        if pred == '' and x != '':\n",
    "                            deletion_added += 1\n",
    "            return error / elements, (deletion_undetected + deletion_added + uncorrectly_undeleted) / elements,\\\n",
    "                    errors_corrected, errors_added, correctly_undeleted, deletion_undetected,\\\n",
    "                        deletion_added, uncorrectly_undeleted\n",
    "\n",
    "        def score_dummy(self, test):\n",
    "            error = 0\n",
    "            deletion_errors = 0\n",
    "            elements = 0\n",
    "            for word in test:\n",
    "                base = [letter for (letter, _) in word]\n",
    "                truth = [tag for (_, tag) in word]\n",
    "                elements += len(truth)\n",
    "                for x, y in zip(base, truth):\n",
    "                    if x != y:\n",
    "                        error += 1\n",
    "                        if x == '':\n",
    "                            deletion_errors += 1\n",
    "            return error / elements, deletion_errors / elements\n",
    "        \n",
    "        def results_hmm(self, test):\n",
    "            viterbi_error_test, vit_noise_errors, vit_corrected, vit_added, vit_noise,\\\n",
    "            vit_undet_noise, vit_add_noise, vit_unc_undel = self.score_viterbi(test)\n",
    "            error_dummy, del_errors_dummy = self.score_dummy(test)\n",
    "            print(\"Error viterbi = {:.2%}, {:.2%} errors for correcting noisy deletions.\\\n",
    "            \\n{} errors corrected, {} errors added, {} deletion correctly corrected, {} deletions not detected,\\\n",
    "            \\n{} deletions added, {} deletions detected but uncorrectly corrected.\"\n",
    "                  .format(viterbi_error_test, vit_noise_errors, vit_corrected, vit_added, vit_noise,\\\n",
    "                          vit_undet_noise, vit_add_noise, vit_unc_undel))\n",
    "            print(\"Error with nothing changed = {:.2%}, noisy deletion errors = {:.2%}\"\n",
    "                  .format(error_dummy, del_errors_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new2_train10 = noisy_deletions(train10, 0.05)\n",
    "new2_test10 = noisy_deletions(test10, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM created with: \n",
      "26 states\n",
      "27 observations\n"
     ]
    }
   ],
   "source": [
    "obs_list, state_list = distrib_x_y_data(new2_train10)\n",
    "\n",
    "hmm4 = HMM4(state_list, obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm4.init_parameters(new2_train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error viterbi = 9.78%, 3.33% errors for correcting noisy deletions.            \n",
      "460 errors corrected, 104 errors added, 117 deletion correctly corrected, 0 deletions not detected,            \n",
      "0 deletions added, 244 deletions detected but uncorrectly corrected.\n",
      "Error with nothing changed = 14.64%, noisy deletion errors = 4.93%\n"
     ]
    }
   ],
   "source": [
    "hmm4.results_hmm(new2_test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remarks\n",
    "\n",
    "It works suprisingly well, but it probably is because there is problem in the representation of deleted characters. It kind of transforms the deletion problem in a substitution problem with '' being a special character only present in observations.\n",
    "\n",
    "Since we want to keep the tag corresponding to the deleted character, we end up making it trivial for the HMM to detect deletions since the observation is ''.\n",
    "\n",
    "Instead, we should try to find a way to keep the tag, AND, directly reduce the size of the word for us to be oblivious to whether a letter is missing.\n",
    "\n",
    "We thought about it but didn't manage to find a good way to overcome this limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TC4-tp2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
